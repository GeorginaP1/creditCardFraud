{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection Project\n",
    "### Project Summary\n",
    "This project aims to train a classifier that can robustly detect fraudulent credit card transactions\n",
    "### Dataset\n",
    "An open source dataset containing credit card transactions from European cardholders in September 2013.\n",
    "\n",
    "### Notes:\n",
    "- The dataset is unbalanced and only contains 0.172% fraudulent transactions\n",
    "\n",
    "\n",
    "- The dataset is anonymised and transformed into PCA data, original features are confidential\n",
    "\n",
    "- Features V1, ..., V28 = PCA, 29 = Transaction Amount, 30 = Class Label \n",
    "\n",
    "- Users are recommended to use Area Under the Precision-Recall Curve (AUPRC) to improve accuracy due to issues from the imbalanced dataset  \n",
    "\n",
    "### Acknowledgements\n",
    "This project was completed with help from a tutorial by Pranjal Saxena found __[here](https://towardsdatascience.com/credit-card-fraud-detection-using-machine-learning-python-5b098d4a8edc)__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Necessary library imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Packages related to general operating system & warnings\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Packages related to data importing, manipulation, exploratory data #analysis, data understanding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from termcolor import colored as cl # text customization\n",
    "#Packages related to data visualizaiton\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Setting plot sizes and type of plot\n",
    "plt.rc(\"font\", size=14)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.gray()\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.preprocessing import  PolynomialFeatures, KBinsDiscretizer, FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, OrdinalEncoder\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa as tsa\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz, export_text\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor,RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor \n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Dataset import\n",
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Investigating the distribution of data (bisaed towards clean transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of transactions: 284807\u001b[0m\n",
      "Total num of normal transactions: 284315\u001b[0m\n",
      "Total num of fraudulent transactions: 492\u001b[0m\n",
      "Percentage of fraudulent transactions: 0.17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Total_transactions = len(data)\n",
    "normal = len(data[data.Class == 0])\n",
    "fraudulent = len(data[data.Class == 1])\n",
    "fraud_percent = round(fraudulent/normal*100, 2)\n",
    "print(cl('Total num of transactions: {}'.format(Total_transactions)))\n",
    "print(cl('Total num of normal transactions: {}'.format(normal)))\n",
    "print(cl('Total num of fraudulent transactions: {}'.format(fraudulent)))\n",
    "print(cl('Percentage of fraudulent transactions: {}'.format(fraud_percent)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scaling and Preparing Data\n",
    "\n",
    "- Feature 29, the transaction amount, has a significantly large range. This would affect the classifier results so the variable will be scaled. Scaling the variable will reduce the range, while preserving the spread of data.\n",
    "\n",
    "- Notes on the scaling process: A StandardScaler() is used with the fit_transform() function. When scaling data with a Scikit-learn StandardScaler(), you can either use the fit_transform() or the transform() method. fit_transform() is used on training data, and calculates the mean and variance of each feature to center and scale the data. transform() however uses the pre-calculated feature mean and variances to scale test data, keeping the test data consistent with the training set.\n",
    "\n",
    "- Feature 0 is dropped from the data set as it isn't a useful feature for the models.\n",
    "\n",
    "- Duplicate transactions are removed to improve model efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the 'amount' feature\n",
    "sc = StandardScaler()\n",
    "amount = data['Amount'].values\n",
    "data['Amount'] = sc.fit_transform(amount.reshape(-1, 1))\n",
    "\n",
    "# Drop the 'time' feature\n",
    "data.drop(['Time'], axis=1, inplace=True)\n",
    "\n",
    "# Drop any duplicate transactions\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Splitting the data into Train & Test sets\n",
    "\n",
    "Models are trained on a subset of the data, known as the train data. Model performance is then evaluated using the test data to assess accuracy.\n",
    "\n",
    "- First the independent (main feature data) and dependent (class values) variables are separated, then the data is split into corresponding test and train sets. Performing the split in one line of code ensures the data and class labels remain consistent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the data and class labels\n",
    "pure_data = data.drop('Class', axis = 1).values\n",
    "labels = data['Class'].values\n",
    "\n",
    "# Split the data and labels into train and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(pure_data, labels, test_size=0.25, random_state=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
