{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection Project\n",
    "### Project Summary\n",
    "This project aims to train a classifier that can robustly detect fraudulent credit card transactions\n",
    "### Dataset\n",
    "An open source dataset containing credit card transactions from European cardholders in September 2013.\n",
    "\n",
    "### Notes:\n",
    "- The dataset is unbalanced and only contains 0.172% fraudulent transactions\n",
    "\n",
    "\n",
    "- The dataset is anonymised and transformed into PCA data, original features are confidential\n",
    "\n",
    "- Features V1, ..., V28 = PCA, 29 = Transaction Amount, 30 = Class Label \n",
    "\n",
    "- Users are recommended to use Area Under the Precision-Recall Curve (AUPRC) to improve accuracy due to issues from the imbalanced dataset  \n",
    "\n",
    "### Acknowledgements\n",
    "This project was completed with help from a tutorial by Pranjal Saxena found __[here](https://towardsdatascience.com/credit-card-fraud-detection-using-machine-learning-python-5b098d4a8edc)__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Necessary library imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Packages related to general operating system & warnings\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Packages related to data importing, manipulation, exploratory data #analysis, data understanding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from termcolor import colored as cl # text customization\n",
    "#Packages related to data visualizaiton\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Setting plot sizes and type of plot\n",
    "plt.rc(\"font\", size=14)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.gray()\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.preprocessing import  PolynomialFeatures, KBinsDiscretizer, FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, OrdinalEncoder\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa as tsa\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz, export_text\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor,RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor \n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Dataset import\n",
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Investigating the distribution of data (bisaed towards clean transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of transactions: 284807\u001b[0m\n",
      "Total num of normal transactions: 284315\u001b[0m\n",
      "Total num of fraudulent transactions: 492\u001b[0m\n",
      "Percentage of fraudulent transactions: 0.17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Total_transactions = len(data)\n",
    "normal = len(data[data.Class == 0])\n",
    "fraudulent = len(data[data.Class == 1])\n",
    "fraud_percent = round(fraudulent/normal*100, 2)\n",
    "print(cl('Total num of transactions: {}'.format(Total_transactions)))\n",
    "print(cl('Total num of normal transactions: {}'.format(normal)))\n",
    "print(cl('Total num of fraudulent transactions: {}'.format(fraudulent)))\n",
    "print(cl('Percentage of fraudulent transactions: {}'.format(fraud_percent)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scaling and Preparing Data\n",
    "\n",
    "- Feature 29, the transaction amount, has a significantly large range. This would affect the classifier results so the variable will be scaled. Scaling the variable will reduce the range, while preserving the spread of data.\n",
    "\n",
    "- Notes on the scaling process: A StandardScaler() is used with the fit_transform() function. When scaling data with a Scikit-learn StandardScaler(), you can either use the fit_transform() or the transform() method. fit_transform() is used on training data, and calculates the mean and variance of each feature to center and scale the data. transform() however uses the pre-calculated feature mean and variances to scale test data, keeping the test data consistent with the training set.\n",
    "\n",
    "- Feature 0 is dropped from the data set as it isn't a useful feature for the models.\n",
    "\n",
    "- Duplicate transactions are removed to improve model efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the 'amount' feature\n",
    "sc = StandardScaler()\n",
    "amount = data['Amount'].values\n",
    "data['Amount'] = sc.fit_transform(amount.reshape(-1, 1))\n",
    "\n",
    "# Drop the 'time' feature\n",
    "data.drop(['Time'], axis=1, inplace=True)\n",
    "\n",
    "# Drop any duplicate transactions\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Splitting the data into Train & Test sets\n",
    "\n",
    "Models are trained on a subset of the data, known as the train data. Model performance is then evaluated using the test data to assess accuracy.\n",
    "\n",
    "- First the independent (main feature data) and dependent (class values) variables are separated, then the data is split into corresponding test and train sets. Performing the split in one line of code ensures the data and class labels remain consistent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the data and class labels\n",
    "pure_data = data.drop('Class', axis = 1).values\n",
    "labels = data['Class'].values\n",
    "\n",
    "# Split the data and labels into train and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(pure_data, labels, test_size=0.25, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Model 1: Decision Tree\n",
    "\n",
    "A desicion tree classifier is a form of supervised machine learning that uses a rule-based approach to make classification decisions.\n",
    "\n",
    "The feature space is continually broken down into sets of rules until rules/data points run out. This results in a decision tree where each node corresponds to a class in the feature space. All pure leaf nodes (ones at the deepest layer of the tree that don't split data further) should be assigned to one class. In cases of mixed leaf nodes where the data points have varying classes, the most common class is selected.\n",
    "\n",
    "The final tree won't be fully optimised as this is an NP-hard problem, instead a greedy approach is used. At each split, the algorithm tries to divide the dataset into the smallest possible subset and minimise the loss function.\n",
    "\n",
    "#### Model performance: \n",
    "\n",
    "- Decision trees are prone to overfitting. By splitting the feature space until pure leaf nodes are reached, you can end up with a large tree that's too complex. The opposite can also happen where a tree ends up too small and underfits the data. \n",
    "\n",
    "- Decision trees aren't robust. Small changes in the data can drastically effect the produced tree, and the models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Decision Tree model: 0.9991729061466132\n",
      "F1 score of the Decision Tree model: 0.7574468085106382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[68770,    18],\n",
       "       [   39,    89]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the model with Scikit-learn\n",
    "DT = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
    "\n",
    "# Fit the model to the train data\n",
    "DT.fit(train_data, train_labels)\n",
    "\n",
    "# Run the model on the test data\n",
    "dt_yhat = DT.predict(test_data)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print('Accuracy score of the Decision Tree model: {}'.format(accuracy_score(test_labels, dt_yhat)))\n",
    "\n",
    "# Evaluate the F1 score of the model\n",
    "    # Standard accuracy isn't a good representaion if the dataset is class-unbalanced\n",
    "    # F1 score combines model precision & recall for a more accurate performance assessment \n",
    "print('F1 score of the Decision Tree model: {}'.format(f1_score(test_labels, dt_yhat)))\n",
    "\n",
    "# Print the confusion matrix\n",
    "confusion_matrix(test_labels, dt_yhat, labels=[0,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Model 2: K-Nearest Neighbour"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
